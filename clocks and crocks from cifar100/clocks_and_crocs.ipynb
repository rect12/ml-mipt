{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pygpu\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация изображений\n",
    "\n",
    "Требуется написать программу, отличающую картинки размером 32х32 с часами от картинок с крокодилами. \n",
    "\n",
    "В предложенном датасете находилось 1000 картинок (500 с часами и 500 с крокодилами). Для обучения сети были взяты 800 картинок, на оставшихся 200 картинок же проводилось тестирование модели. Так как датасет довольно мал, то используя 800 тренировочных картинок были получены дополнительные примеры, путем поворота их на рандомный угол, а также их отзеркаливание. Нормирование данных также немного улучшило результат. \n",
    "\n",
    "Для вдохновения были использованы следующие ссылки:\n",
    "\n",
    "1. Курс машинного обучения в МФТИ. Пример написания сверточной сети на lasagne для классификации датасета cifar10. Брал отсюда общую структуру сети на lasagne с theano.\n",
    "https://github.com/ml-mipt/ml-mipt-part1/blob/master/2017/home_work/hw6/cifar.ipynb\n",
    "2. Нейронная сеть на keras для классификации датасета cifar10. Подсмотрел тут конкруцию самой сети.\n",
    "http://parneetk.github.io/blog/cnn-cifar10/\n",
    "3. Нейронные сети на torch и lasagne для классификации картинок cifar10. Брал отсюда некоторые идеи, которые помогли улучшить качество.\n",
    "http://torch.ch/blog/2015/07/30/cifar.html\n",
    "https://github.com/Lasagne/Recipes/blob/master/papers/deep_residual_learning/Deep_Residual_Learning_CIFAR-10.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda0: GRID K520 (0000:00:03.0)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from scipy.misc import imrotate\n",
    "from scipy.stats import norm\n",
    "\n",
    "def load_data():\n",
    "    clocks = os.listdir('./clocks')\n",
    "    crocodiles = os.listdir('./crocodiles')\n",
    "    \n",
    "    xs = []\n",
    "    ys = []\n",
    "    for image in clocks:\n",
    "        with open('clocks/'+image, 'rb') as f:\n",
    "            xs.append(plt.imread(f, 'F').transpose(2,0,1))    \n",
    "            ys.append(0)\n",
    "    for image in crocodiles:\n",
    "        with open('crocodiles/'+image, 'rb') as f:\n",
    "            xs.append(plt.imread(f, 'F').transpose(2,0,1))\n",
    "            ys.append(1)\n",
    "    \n",
    "    for train_ind, test_ind in (StratifiedShuffleSplit(n_splits=1, random_state=0, test_size=0.2).split(xs, ys)):\n",
    "        X_train = [xs[ind] for ind in train_ind]/ np.float32(255)\n",
    "        Y_train = [ys[ind] for ind in train_ind]\n",
    "        X_test = [xs[ind] for ind in test_ind]/ np.float32(255)\n",
    "        Y_test = [ys[ind] for ind in test_ind]\n",
    "    \n",
    "    # добавим повернутые на рандомный угол изображения\n",
    "    for image in clocks:\n",
    "        with open('clocks/'+image, 'rb') as f:\n",
    "            np.append(X_train, imrotate(plt.imread(f, 'F'), norm.rvs(0, 0.5), interp='nearest').transpose(2,0,1)/np.float32(255))\n",
    "            np.append(Y_train, 0)\n",
    "    for image in crocodiles:\n",
    "        with open('crocodiles/'+image, 'rb') as f:\n",
    "            np.append(X_train,imrotate(plt.imread(f, 'F'), norm.rvs(0, 0.5), interp='nearest').transpose(2,0,1)/np.float32(255))\n",
    "            np.append(Y_train, 1)\n",
    "    \n",
    "    # создание зеркальных изображений\n",
    "    X_train_flip = X_train[:,:,:,::-1]\n",
    "    Y_train_flip = Y_train\n",
    "    X_train = np.concatenate((X_train,X_train_flip),axis=0)\n",
    "    Y_train = np.concatenate((Y_train,Y_train_flip),axis=0)\n",
    "    \n",
    "    return (\n",
    "        lasagne.utils.floatX(X_train),\n",
    "        Y_train,\n",
    "        lasagne.utils.floatX(X_test),\n",
    "        Y_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Классы разделились в тестовой выборке равномерно\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_data()\n",
    "print(\"Классы разделились в тестовой выборке равномерно\")\n",
    "print(y_test.count(0))\n",
    "print(y_test.count(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем саму сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var=None):\n",
    "    net = lasagne.layers.InputLayer(shape=(None, 3, 32, 32), input_var=input_var)\n",
    "    \n",
    "    net = lasagne.layers.BatchNormLayer(net)\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(3,3), stride=(1,1), pad='same', b=None)\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(3,3), stride=(1,1), pad='same', b=None)\n",
    "    net = lasagne.layers.MaxPool2DLayer(net, 2)\n",
    "    net = lasagne.layers.DropoutLayer(net, p=0.4)\n",
    "    \n",
    "    net = lasagne.layers.BatchNormLayer(net)\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=64, filter_size=(3,3), stride=(1,1), pad='same', b=None)\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=64, filter_size=(3,3), stride=(1,1), pad='same', b=None)\n",
    "    net = lasagne.layers.MaxPool2DLayer(net, 2)\n",
    "    net = lasagne.layers.DropoutLayer(net, p=0.4)\n",
    "\n",
    "    net = lasagne.layers.BatchNormLayer(net)\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=128, filter_size=(3,3), stride=(1,1), pad='same', b=None)\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=128, filter_size=(3,3), stride=(1,1), pad='same', b=None)\n",
    "    net = lasagne.layers.MaxPool2DLayer(net, 2)\n",
    "    net = lasagne.layers.DropoutLayer(net, p=0.4)\n",
    "    \n",
    "    net = lasagne.layers.BatchNormLayer(net)\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=256, filter_size=(5,5), stride=(1,1), pad='same', b=None)\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=256, filter_size=(5,5), stride=(1,1), pad='same', b=None)\n",
    "    net = lasagne.layers.MaxPool2DLayer(net, 2)\n",
    "    net = lasagne.layers.DropoutLayer(net, p=0.5)\n",
    "    \n",
    "    net = lasagne.layers.DenseLayer(net, num_units=512, nonlinearity=lasagne.nonlinearities.elu)\n",
    "    net = lasagne.layers.DenseLayer(net, num_units=256, nonlinearity=lasagne.nonlinearities.elu)\n",
    "    \n",
    "    net = lasagne.layers.DenseLayer(net, num_units=2, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_X = T.tensor4(\"X\")\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = build_cnn(input_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[beta, gamma, W, W, beta, gamma, W, W, beta, gamma, W, W, beta, gamma, W, W, W, b, W, b, W, b]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = lasagne.layers.get_output(net)\n",
    "all_weights = lasagne.layers.get_all_params(net, trainable=True)\n",
    "print(all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Используем l2 регуляризацию, чтобы сеть не переобучалась так сильно. В качестве метода оптимизации используем momentum. Он быстрее сходится, чем adam, а так же показывает немного лучше результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import get_all_layers\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted, target_y).mean()\n",
    "loss += lasagne.regularization.regularize_layer_params(get_all_layers(net), lasagne.regularization.l2) * 0.0001\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted, target_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_lr = 0.01\n",
    "updates = lasagne.updates.momentum(loss, all_weights, learning_rate=curr_lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fun = theano.function([input_X, target_y], [loss, accuracy], updates=updates, allow_input_downcast=True)\n",
    "accuracy_fun = theano.function([input_X, target_y], accuracy, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch iterator\n",
    "\n",
    "Также мы здесь шафлим батчи, это улучшает качество предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение сети.\n",
    "Learning rate уменьшается на 12 эпохе, это число взято из экспериментов и наблюдения за процессом обучения:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30 took 10.398s\n",
      "  training loss (in-iteration):\t\t0.953824\n",
      "  train accuracy:\t\t69.88 %\n",
      "  validation accuracy:\t\t81.25 %\n",
      "Epoch 2 of 30 took 10.367s\n",
      "  training loss (in-iteration):\t\t0.723020\n",
      "  train accuracy:\t\t83.81 %\n",
      "  validation accuracy:\t\t82.29 %\n",
      "Epoch 3 of 30 took 10.387s\n",
      "  training loss (in-iteration):\t\t0.673545\n",
      "  train accuracy:\t\t86.81 %\n",
      "  validation accuracy:\t\t86.98 %\n",
      "Epoch 4 of 30 took 10.382s\n",
      "  training loss (in-iteration):\t\t0.643681\n",
      "  train accuracy:\t\t88.56 %\n",
      "  validation accuracy:\t\t87.50 %\n",
      "Epoch 5 of 30 took 10.382s\n",
      "  training loss (in-iteration):\t\t0.592385\n",
      "  train accuracy:\t\t89.75 %\n",
      "  validation accuracy:\t\t84.38 %\n",
      "Epoch 6 of 30 took 16.905s\n",
      "  training loss (in-iteration):\t\t0.587309\n",
      "  train accuracy:\t\t89.50 %\n",
      "  validation accuracy:\t\t86.98 %\n",
      "Epoch 7 of 30 took 20.636s\n",
      "  training loss (in-iteration):\t\t0.588497\n",
      "  train accuracy:\t\t90.25 %\n",
      "  validation accuracy:\t\t85.42 %\n",
      "Epoch 8 of 30 took 20.672s\n",
      "  training loss (in-iteration):\t\t0.546800\n",
      "  train accuracy:\t\t92.44 %\n",
      "  validation accuracy:\t\t90.62 %\n",
      "Epoch 9 of 30 took 20.482s\n",
      "  training loss (in-iteration):\t\t0.545619\n",
      "  train accuracy:\t\t91.88 %\n",
      "  validation accuracy:\t\t86.98 %\n",
      "Epoch 10 of 30 took 20.616s\n",
      "  training loss (in-iteration):\t\t0.527433\n",
      "  train accuracy:\t\t92.50 %\n",
      "  validation accuracy:\t\t90.62 %\n",
      "Epoch 11 of 30 took 17.504s\n",
      "  training loss (in-iteration):\t\t0.524073\n",
      "  train accuracy:\t\t92.88 %\n",
      "  validation accuracy:\t\t91.15 %\n",
      "Epoch 12 of 30 took 10.377s\n",
      "  training loss (in-iteration):\t\t0.483419\n",
      "  train accuracy:\t\t94.75 %\n",
      "  validation accuracy:\t\t89.06 %\n",
      "Epoch 13 of 30 took 10.387s\n",
      "  training loss (in-iteration):\t\t0.478326\n",
      "  train accuracy:\t\t95.12 %\n",
      "  validation accuracy:\t\t91.67 %\n",
      "Epoch 14 of 30 took 10.378s\n",
      "  training loss (in-iteration):\t\t0.464037\n",
      "  train accuracy:\t\t95.06 %\n",
      "  validation accuracy:\t\t91.67 %\n",
      "Epoch 15 of 30 took 10.393s\n",
      "  training loss (in-iteration):\t\t0.469849\n",
      "  train accuracy:\t\t94.81 %\n",
      "  validation accuracy:\t\t89.06 %\n",
      "Epoch 16 of 30 took 14.775s\n",
      "  training loss (in-iteration):\t\t0.463189\n",
      "  train accuracy:\t\t95.06 %\n",
      "  validation accuracy:\t\t90.10 %\n",
      "Epoch 17 of 30 took 21.611s\n",
      "  training loss (in-iteration):\t\t0.464180\n",
      "  train accuracy:\t\t94.94 %\n",
      "  validation accuracy:\t\t90.62 %\n",
      "Epoch 18 of 30 took 21.609s\n",
      "  training loss (in-iteration):\t\t0.447577\n",
      "  train accuracy:\t\t95.62 %\n",
      "  validation accuracy:\t\t89.58 %\n",
      "Epoch 19 of 30 took 21.216s\n",
      "  training loss (in-iteration):\t\t0.435424\n",
      "  train accuracy:\t\t96.56 %\n",
      "  validation accuracy:\t\t86.46 %\n",
      "Epoch 20 of 30 took 21.567s\n",
      "  training loss (in-iteration):\t\t0.420216\n",
      "  train accuracy:\t\t97.00 %\n",
      "  validation accuracy:\t\t92.19 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6907346f361e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 30 \n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch == 11:\n",
    "        curr_lr = curr_lr/2\n",
    "        updates = lasagne.updates.momentum(loss, all_weights, learning_rate=curr_lr, momentum=0.9)\n",
    "        train_fun = theano.function([input_X, target_y], [loss, accuracy], updates=updates, allow_input_downcast=True)    \n",
    "\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, batch_size, True):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(epoch+1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на итоговый результат сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t93.00 %\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 50):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили 93%, то есть сеть правильно классифицировала 188 картинок из тестовой выборки:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
